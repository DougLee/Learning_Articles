# 搭建Hadoop集群环境

系统: CentOS 7, 三台.

Hadoop版本: Hadoop 2.6.5

## step 1. 搭建虚拟机

用三台CentOS7虚拟机来搭建集群环境.一台master 两台作为slave.

具体新建虚机过程略.

## step 2. 设定hostname

为了方便后续的操作把三台虚拟机的hostname分别设置成master和slave1, slave2.

```bash
# 1 在三台机器上分别执行
hostnamectl --static set-hostname master
hostnamectl --static set-hostname slave1
hostnamectl --static set-hostname slave2
# 2 使用如下命令检查hostname
hostname
# 3. 重启机器
reboot
```

## step 3 设置网络以及hosts

把三台机器的IP分别设置为 192.168.199.10, 192.168.199.11, 192.168.199.12. 虚拟机的网络连接使用桥接模式

```bash
vi /etc/sysconfig/network-scripts/ifg-ens33
# 添加如下配置
# 192.168.199.10的配置
BOOTPROTO=static
ONBOOT=yes
IPADDR=192.168.199.10
NETMASK=255.255.255.0
GATEWAY=192.168.199.1
# 192.168.199.11的配置
BOOTPROTO=static
ONBOOT=yes
IPADDR=192.168.199.11
NETMASK=255.255.255.0
GATEWAY=192.168.199.1
# 192.168.199.12的配置
BOOTPROTO=static
ONBOOT=yes
IPADDR=192.168.199.12
NETMASK=255.255.255.0
GATEWAY=192.168.199.1
```

修改三台机器的hosts

```bash
vi /etc/hosts
# 添加如下配置
192.168.199.10 master
192.168.199.11 slave1
192.168.199.12 slave2
```

## step 4 关闭防火墙

```bash
# 1. 查看防火墙状态
systemctl status firewalld
# 2. 如果状态为开启, 则关闭
systemctl stop firewalld
# 3. 关闭开启自动开启防火墙功能
systemctl disable firewalld
```

## step 5 配置ssh免密登录

在三台机器上执行如下命令

```bash
# 1. 生成公钥
ssh-keygen -t rsa
# 2. 拷贝到三台机器上
ssh-copy-id root@master
ssh-copy-id root@slave1
ssh-copy-id root@slave2

# 3. 验证登录是否成功
ssh slave1 可以直接登录
```

## step 6 安装配置JDK

以下步骤在三台机器上执行

1. 下载 JDK  [下载地址](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)

2. 安装jdk yum install -y xxx.jdk

   ```bash
   mkdir /software
   把下载的jdk拷贝到/software中
   yum install -y jdk.xxx.rpm
   ```

   ​

3. 设置JAVA_HOME

```bash
cd /usr/java/jdk.xxx
vi /etc/profile
在文件中添加如下配置
```

```bash
export JAVA_HOME=/usr/local/jdk1.8.0_111
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

然后

```bash
source /etc/profile
```

4. 验证java 是否安装成功

```
java -version
```

## step 7 安装配置Hadoop

以下步骤在三台机器上执行

下载Hadoop 2.6.5 地址  [下载地址](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz) 

```bash
# 1. 新建一个目录存放下载的安装包
mkdir /software 
# 2. 解压, 把hadoop安装在/usr路径下
cd /usr
cp /software/hadoop.xxx /usr
tar -zxvf hadoop.xxx.tar.gz
# 3. 创建hadoop需要的几个路径
mkdir -p /data/hadoop/namenode
mkdir -p /data/hadoop/data
mkdir -p /data/hadoop/tmp
# 4. 配置Hadoop, 修改core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml四个文件
cd /usr/hadoop-2.6.5/etc/hadoop
4个xml的配置内容在下方
# 5. 配置slaves
vi slaves
# 添加如下内容
slave1
slave2
# 6. 配置masters
vi masters
# 添加如下内容
master
# 7. 配置hadoop-env.sh, 在文件最后添加JAVA_HOME的配置
vi hadoop-env.sh
# 文件最后添加
export JAVA_HOME=/usr/java/jdk.xxx
# 8. 配置Hadoop的环境变量
vi /etc/profile
# 在文件最后添加
export HADOOP_HOME=/usr/hadoop-2.6.5
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

# 9. 格式化
cd /usr/hadoop.xxx/bin
hdfs namenode -format
# 10. 启动
cd ../sbin
# 启动hdfs
./start-dfs.sh
# 启动yarn
./start-yarn.sh
```

四个文件分别修改成如下:

core-site.xml

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
		<description>namenode通信地址</description>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop/tmp</value>
		<description>临时文件存储路径</description>
    </property>
</configuration>
```

hdfs-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<property>
		<name>dfs.replication</name>
		<value>2</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/data/hadoop/namenode</value>
		<final>true</final>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/data/hadoop/data</value>
		<final>true</final>
	</property>
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>master:9001</value>
	</property>
	<property>
		<name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
</configuration>
```

mapred-site.xml

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

</configuration>
```

yarn-site.xml

```xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

<!-- Site specific YARN configuration properties -->
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>master:8040</value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>master:8030</value>
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>master:8088</value>
		<description>ResourceManager对外web ui地址。用户可通过该地址在浏览器中查看集群各类信息</description>
	</property>
	<property>
		<name>yarn.resourcemanager.resource-tracker.address</name>
		<value>master:8025</value>
	</property>
	<property>
		<name>yarn.resourcemanager.admin.address</name>
		<value>master:8141</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
		<description>NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序</description>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.acl.enable</name>
		<value>false</value>
	</property>
	<property>
		<name>yarn.admin.acl</name>
		<value>*</value>
	</property>
</configuration>
```



## 其他用到的命令

```bash
# 把修改的配置复制到slave1和slave2
scp -r ./* root@slave1:/usr/hadoop.xxx/etc/hadoop
scp -r ./* root@slave2:/usr/hadoop.xxx/etc/hadoop
```



   

